import { getCustomStaticPath } from '@/utils/getCustomStaticPath';

export const meta = {
  title: 'AI/ML Predictions',
  description: 'Learn how to set up AI/ML Predictions',
  platforms: [
    'angular',
    'flutter',
    'javascript',
    'nextjs',
    'react',
    'vue'
  ]
};

export const getStaticPaths = async () => {
  return getCustomStaticPath(meta.platforms);
};

export function getStaticProps(context) {
  return {
    props: {
      meta
    }
  };
}

<Callout warning>

**Under active development:** The `addOutput` method for Amplify Gen 2 is under active development. The experience may change between versions of `@aws-amplify/backend`. Try it out and provide feedback at https://github.com/aws-amplify/amplify-backend/issues/new/choose

</Callout>

Amplify provides provides a solution for using AI and ML cloud services to enhance your application. Some supported use cases:
<ul>
<li>
Convert text to speech
</li>
<li>
Translate text from one language to another
</li>
<li>
Identify text from an image
</li>
<li>
Identify entities from an image
</li>
<li>
Identify real world objects from an image
</li>
</ul>

Predictions is broadly organized into 3 key use cases - Identify, Convert, and Interpret - which are available in the client API as well as CLI workflows.
<ul>
<li>
`Identify` will find text (words, tables, pages from a book), entities (faces and/or celebrities) from images. You can also identify real world landmarks or objects such as chairs, desks, etc. which are referred to as “labels” from images.
</li>
<li>
`Convert` allows you to translate text from one source language to a target language. You can also generate speech audio from text input. Lastly, you can take an audio input and transcribe it using a websocket stream.
</li>
<li>
`Interpret` allows you to analyze text for language, entities (places, people), key phrases, sentiment (positive, neutral, negative), and syntax (pronouns, verbs, adjectives).
</li>
</ul>
Some common use cases are listed below, as well as an advanced workflow which allows you to perform dynamic image indexing from a connected s3 bucket.

Predictions comes with built-in support for [Amazon Translate](https://docs.aws.amazon.com/translate/latest/dg/what-is.html), [Amazon Polly](https://docs.aws.amazon.com/polly/latest/dg/what-is.html), [Amazon Transcribe](https://docs.aws.amazon.com/transcribe/latest/dg/what-is-transcribe.html), [Amazon Rekognition](https://docs.aws.amazon.com/rekognition/latest/dg/what-is.html), [Amazon Textract](https://docs.aws.amazon.com/textract/latest/dg/what-is.html), and [Amazon Comprehend](https://docs.aws.amazon.com/comprehend/latest/dg/what-is.html).

## Text to speech

### Set up the backend

To convert a text to speech, we are going to use a mutation that will trigger a function to use Amazon Polly to generate an audio from a text input

First, update the data definition as below

```ts title="amplify/data/resource.ts"

import { type ClientSchema, a, defineData, defineFunction } from '@aws-amplify/backend';

export const convertTextToSpeech = defineFunction({
  entry: './convertTextToSpeech.ts'
})

const schema = a.schema({
  Todo: a.model({
    content: a.string(),
  }).authorization([a.allow.owner(), a.allow.public().to(['read'])]),

  

  convertTextToSpeech: a.mutation()
    .arguments({
      text: a.string().required()
    })
    .returns(a.string().required())
    .authorization([a.allow.public()])
    .handler(a.handler.function(convertTextToSpeech))
});

export type Schema = ClientSchema<typeof schema>;

export const data = defineData({
  schema,
  authorizationModes: {
    defaultAuthorizationMode: 'apiKey',
    // API Key is used for a.allow.public() rules
    apiKeyAuthorizationMode: {
      expiresInDays: 30,
    },
  },
});

```

Next, Create the function definition at amplify/data/convertTextToSpeech.ts.

```ts title="amplify/data/convertTextToSpeech.ts"

import { Schema } from "./resource";
import { PollyClient, StartSpeechSynthesisTaskCommand } from '@aws-sdk/client-polly'
import { env } from '$amplify/env/convertTextToSpeech'

export const handler: Schema["convertTextToSpeech"]["functionHandler"] = async (event) => {
  const client = new PollyClient()
  const task = new StartSpeechSynthesisTaskCommand({
    OutputFormat: 'mp3',
    SampleRate: '8000',
    Text: event.arguments.text,
    TextType: 'text',
    VoiceId: 'Amy',
    OutputS3BucketName: env.predictionsforgen2_BUCKET_NAME,
    OutputS3KeyPrefix: 'public/'
  })
  const result = await client.send(task)

  return result.SynthesisTask?.OutputUri?.replace(
    'https://s3.us-east-1.amazonaws.com/' + env.predictionsforgen2_BUCKET_NAME + '/public/', "") ?? ""
}

```

Lastly, Add the function to your backend & configure the IAM Policy for the asynchronous synthesis functionality.

```ts title="amplify/backend.ts"

import { defineBackend } from '@aws-amplify/backend';
import { auth } from './auth/resource';
import { data, convertTextToSpeech } from './data/resource';
import { PolicyStatement } from 'aws-cdk-lib/aws-iam';
import { storage } from './storage/resource';


const backend = defineBackend({
  auth,
  data,
  storage,
  convertTextToSpeech
});



backend.convertTextToSpeech.resources.lambda.addToRolePolicy(new PolicyStatement({
  actions: ['polly:StartSpeechSynthesisTask'],
  resources: [The_ARN_Of_Amazon Polly_Resource_Type]
}))

```


### Working with the API

Generate an audio buffer for playback from a text input.

```
    const { data } = await client.mutations.convertTextToSpeech({
      text: "Welcome to Amplify!",
    });
    setFile(data);

```

## Translate language

### Set up the backend

To translate a text to another language, we are going to use a query that takes the following arguments:

<ul>
<li>
A string represnting the sourceLanguage
</li>
<li>
A string represnting the targetLanguage
</li>
<li>
A string represnting the text to be translated
</li>
</ul>

First, update the data definition as below

```ts title="amplify/data/resource.ts"

import { type ClientSchema, a, defineData } from '@aws-amplify/backend';



const schema = a.schema({

  translate: a.query()
    .arguments({
      sourceLanguage: a.string().required(),
      targetLanguage: a.string().required(),
      text: a.string().required()
    })
    .returns(a.string())
    .authorization([a.allow.public()])
    .handler(a.handler.custom({
      dataSource: "TranslateDataSource",
      entry: './translate.js'
    })),

 
});

export type Schema = ClientSchema<typeof schema>;

export const data = defineData({
  schema,
  authorizationModes: {
    defaultAuthorizationMode: 'apiKey',
    // API Key is used for a.allow.public() rules
    apiKeyAuthorizationMode: {
      expiresInDays: 30,
    },
  },
});



```

The query will use a custom handler defined in amplify/data/translate.js.

```js title="amplify/data/translate.js"

export function request(ctx) {
    return {
      method: 'POST',
      resourcePath: '/',
      params: {
        body: {
          SourceLanguageCode: ctx.arguments.sourceLanguage,
          TargetLanguageCode: ctx.arguments.targetLanguage,
          Text: ctx.arguments.text
        },
        headers: {
          'Content-Type': 'application/x-amz-json-1.1',
          'X-Amz-Target': 'AWSShineFrontendService_20170701.TranslateText'
        }
      },
    }
  }
  
  export function response(ctx) {
    return JSON.parse(ctx.result.body).TranslatedText
  }

```

The query will use TranslateDataSource as its data source.

```ts title="amplify/backend.ts"

import { defineBackend } from '@aws-amplify/backend';
import { auth } from './auth/resource';
import { data } from './data/resource';
import { Stack } from 'aws-cdk-lib';
import { PolicyStatement } from 'aws-cdk-lib/aws-iam';



const backend = defineBackend({
  auth,
  data,

});

const dataStack = Stack.of(backend.data)

const translateDataSource = backend.data.addHttpDataSource("TranslateDataSource", `https://translate.${dataStack.region}.amazonaws.com`, {
  authorizationConfig: {
    signingRegion: dataStack.region,
    signingServiceName: 'translate'
  }
})

translateDataSource.grantPrincipal.addToPrincipalPolicy(new PolicyStatement({
  actions: ['translate:TranslateText'],
  resources: [The_ARN_Of_Amazon_Translate_Resource]
}))



```


### Working with the API


```
  const { data }= await client.queries.translate({
    sourceLanguage: "en",
    targetLanguage: "es",
    text: "Welcome to Amplify!",
  });
  console.log(data);

```

## Identify text from Image

To identify a text from an image stored in S3 bucket, we are going to use a query that takes the following arguments:

<ul>
<li>
A string represnting the image file name
</li>
<li>
A string represnting the S3 bucket name
</li>
</ul>


First, update the data definition as below

```ts title="amplify/data/resource.ts"

import { type ClientSchema, a, defineData} from "@aws-amplify/backend";

const schema = a.schema({
  identifyText: a
    .query()
    .arguments({
      path: a.string(),
      bucket: a.string(),
    })
    .returns(a.string())
    .authorization([a.allow.public()])
    .handler(
      a.handler.custom({
        entry: "./identifyText.js",
        dataSource: "RekognitionDataSource",
      })
    ),
});

export type Schema = ClientSchema<typeof schema>;

export const data = defineData({
  schema,
  authorizationModes: {
    defaultAuthorizationMode: "apiKey",
    // API Key is used for a.allow.public() rules
    apiKeyAuthorizationMode: {
      expiresInDays: 30,
    },
  },
});



```

The query will use a custom handler defined in amplify/data/identifyText.js.

```js title="amplify/data/identifyText.js"

export function request(ctx) {
    return {
      method: 'POST',
      resourcePath: '/',
      params: {
        body: {
          Image: {
            S3Object: {
              Bucket: ctx.arguments.bucket,
              Name: ctx.arguments.path
            }
          }
        },
        headers: {
          'Content-Type': 'application/x-amz-json-1.1',
          'X-Amz-Target': 'RekognitionService.DetectText'
        }
      },
    }
  }
  
  export function response(ctx) {
    return JSON.parse(ctx.result.body)
      .TextDetections
      .filter(item => item.Type === 'LINE')
      .map(item => item.DetectedText)
      .join('\n')
      .trim()
  }

```

The query will use RekognitionDataSource as its data source.

```ts title="amplify/backend.ts"

import { defineBackend } from '@aws-amplify/backend';
import { auth } from './auth/resource';
import { data } from './data/resource';
import { Stack } from 'aws-cdk-lib';
import { PolicyStatement } from 'aws-cdk-lib/aws-iam';
import { storage } from './storage/resource';


const backend = defineBackend({
  auth,
  data,
  storage,
});

const dataStack = Stack.of(backend.data)


const rekognitionDataSource = backend.data.addHttpDataSource("RekognitionDataSource", `https://rekognition.${dataStack.region}.amazonaws.com`, {
  authorizationConfig: {
    signingRegion: dataStack.region,
    signingServiceName: 'rekognition'
  }
})

rekognitionDataSource.grantPrincipal.addToPrincipalPolicy(new PolicyStatement({
  actions: ['rekognition:DetectText'],
  resources: [The_ARN_Of_Amazon_Rekognition_Resource] 
}))

backend.storage.resources.bucket.grantRead(rekognitionDataSource.grantPrincipal)


```


### Working with the API

```
  const { data }= await client.queries.identifyText({
    path: photoFileName
     bucket: bucketName
  });
  console.log(data);

```

## Label objects in an image

To detect if an image has objects such as chairs, desks, etc. We are going to use a query that takes the following arguments:

<ul>
<li>
A string represnting the image file name
</li>
<li>
A string represnting the S3 bucket name
</li>
</ul>


First, update the data definition as below

```ts title="amplify/data/resource.ts"

import { type ClientSchema, a, defineData} from "@aws-amplify/backend";

const schema = a.schema({
  identifyLabels: a
    .query()
    .arguments({
      path: a.string(),
      bucket: a.string(),
    })
    .returns(a.string())
    .authorization([a.allow.public()])
    .handler(
      a.handler.custom({
        entry: "./identifyLabels.js",
        dataSource: "RekognitionDataSource",
      })
    ),
});

export type Schema = ClientSchema<typeof schema>;

export const data = defineData({
  schema,
  authorizationModes: {
    defaultAuthorizationMode: "apiKey",
    // API Key is used for a.allow.public() rules
    apiKeyAuthorizationMode: {
      expiresInDays: 30,
    },
  },
});



```

The query will use a custom handler defined in amplify/data/identifyLabels.js.

```js title="amplify/data/identifyLabels.js"

export function request(ctx) {
    return {
      method: 'POST',
      resourcePath: '/',
      params: {
        body: {
          Image: {
            S3Object: {
              Bucket: ctx.arguments.bucket,
              Name: ctx.arguments.path
            }
          }
        },
        headers: {
          'Content-Type': 'application/x-amz-json-1.1',
          'X-Amz-Target': 'RekognitionService.DetectLabels'
        }
      },
    }
  }
  
  export function response(ctx) {
    return JSON.parse(ctx.result.body)
      .Labels
      .map(label => label.Name)
      .join(',')
      .trim()
  }

```

The query will use RekognitionDataSource as its data source.

```ts title="amplify/backend.ts"

import { defineBackend } from '@aws-amplify/backend';
import { auth } from './auth/resource';
import { data } from './data/resource';
import { Stack } from 'aws-cdk-lib';
import { PolicyStatement } from 'aws-cdk-lib/aws-iam';
import { storage } from './storage/resource';


const backend = defineBackend({
  auth,
  data,
  storage,
});

const dataStack = Stack.of(backend.data)


const rekognitionDataSource = backend.data.addHttpDataSource("RekognitionDataSource", `https://rekognition.${dataStack.region}.amazonaws.com`, {
  authorizationConfig: {
    signingRegion: dataStack.region,
    signingServiceName: 'rekognition'
  }
})

rekognitionDataSource.grantPrincipal.addToPrincipalPolicy(new PolicyStatement({
  actions: ['rekognition:DetectLabels'],
  resources: [The_ARN_Of_Amazon_Rekognition_Resource] 
}))

backend.storage.resources.bucket.grantRead(rekognitionDataSource.grantPrincipal)


```


### Working with the API

```
  const { data }= await client.queries.identifyLabels({
    path: photoFileName
     bucket: bucketName
  });
  console.log(data);

```

