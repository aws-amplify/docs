import { getCustomStaticPath } from "@/utils/getCustomStaticPath";

export const meta = {
  title: "Retrieval Augmented Generation",
  description:
    "Learn how to use Amazon AI",
  platforms: [
    "javascript",
    "react-native",
    "angular",
    "nextjs",
    "react",
    "vue",
  ],
};

export const getStaticPaths = async () => {
  return getCustomStaticPath(meta.platforms);
};

export function getStaticProps(context) {
  return {
    props: {
      platform: context.params.platform,
      meta,
    },
  };
}



## Retrieval Augmented Generation


You might not need a traditional RAG setup using a Vector database and embeddings depending on your use case. Here are some methods you can use 

### Passing client-side context

You might  

### Data query Tools

Usually in RAG applications a vector search is performed before calling an LLM and the results of the vector search are passed to the LLM. Tool use on the other hand happens after the LLM is initially called and the LLM responds that it wants to invoke a tool. With Amplify AI, you can give the LLM access to query any data defined in your data definition. 
